---
title: 'Assignment: Ch 7'
author: "Monikrishna Roy"
date: "03/26/2021 (updated: `r Sys.Date()`)"
output: 
  bookdown::html_document2:
    number_sections: FALSE
    toc: yes
    keep_md: yes
---

<style type="text/css">
h1.title {
  text-align: center;
}
h4.author {
  text-align: center;
}
h4.date {
  text-align: center;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(
  fig.path = "README_figs/README-7-"
)
set.seed(03262021)
library(bookdown)
library(purrr)
library(energy)
library(mlbench)
```

## Exercise 7.1

Code for estimating the MSE of the level $k$ trimmed means for random samples of size $20$ generated from a standard Cauchy distribution. Summarized the estimates of MSE in a table for $k = 1, 2, . . . , 9$.

```{r}
m <- 2000
K <- 9
n <- 20
tmean <- matrix(0, m, K)
mse_est <- numeric(K)
mse_se <- numeric(K)
for (k in 1:K) {
  for (i in 1:m) {
    x <- sort(rcauchy(n))
    tmean[i, k] <- mean(x[(k + 1):(n - k)])
  }
  mse_est[k] <- mean(tmean[, k] ^ 2)
  mse_se[k] <- sqrt(sum((tmean[, k] - mean(tmean[, k])) ^ 2)) / m
}
table <- cbind(seq(1:K), round(mse_est, 5), round(mse_se, 5))
colnames(table) <-
  c("k", "Estimated MSE of level k trimmed means", "Standard Error")
knitr::kable(table, caption = 'Estimates of MSE')
```

## Exercise 7.3

Plotting the power curves for the t-test in Example $7.9$ for sample sizes $10, 20, 30, 40,$ and $50$, but omitted the standard error bars. Plotted the curves on the same graph, each in a different color or different line type, and included a legend.

```{r, fig.align='center', fig.cap='Power Curve. Comment: when sample size increases, the power changes more abruptly.'}
n <- seq(10, 50, 10) #sample size
mu <- c(seq(350, 650, 10))
m <- 1000
M <- length(mu)
N <- length(n)
power <- matrix(0, M, N)
for (j in 1:N) {
  for (i in 1:M) {
    mu1 <- mu[i]
    pvalues <- replicate(m, expr = {
      #simulate under alternative mu1
      x <- rnorm(n[j], mean = mu1, sd = 100)
      ttest <- t.test(x, alternative = "greater", mu = 500)
      ttest$p.value
    })
    power[i, j] <- mean(pvalues <= .05)
  }
}
plot(mu, power[, 1],  type = "l",  lty = 2,  col = 1,  main = "Power curve",  xlab = "mu",  ylab = "Power")

lines(mu, power[, 2], lty = 3, col = 2)
lines(mu, power[, 3], lty = 4, col = 3)
lines(mu, power[, 4], lty = 5, col = 4)
lines(mu, power[, 5], lty = 1, col = 5)

legend(
  "topleft",
  c(
    "sample size=10",
    "sample size=20",
    "sample size=30",
    "sample size=40",
    "sample size=50"
  ),
  lty = c(2, 3, 4, 5, 1),
  col = c(1, 2, 3, 4, 5)
)
abline(v = 500, lty = 1)
lines(c(450, 550), c(0.05, 0.05))
```

## Exercise 7.5

Refer to Example 1.6 (run length encoding). Code for simulation to estimate the probability that the observed maximum run length for the fair coin flipping experiment is in $[9, 11]$ in a sample size of $1000$. Estimated the standard error of the maximum run length for this experiment. 
Suppose that you observed 1000 coin flips and the maximum run length was 9. It might be fair because the probability for maximum run length is between 9 and 11 which is around $0.605$.

```{r}
n <- 1000
max_run_len <- replicate(n, expr = {
  x <- rbinom(n, size = 1, prob = .5)
  r <- rle(x)
  mx <- max(r$lengths)
  mx
})

prob <- mean(max_run_len >= 9 & max_run_len <= 11)
se <- sd(max_run_len) / sqrt(n)

cbind(probabilty = prob, se=se)
```

## Exercise 7.6

My t-interval results is larger than the simulation results in Example $7.4$, so itâ€™s more stable for departures from normality.

```{r}
alpha <- 0.05
m <- 1000
n <- 20
qt <- qt(1 - alpha / 2, df = n - 1)
LCL <- replicate(m, expr = {
  x <- rchisq(n, 2)
  return(mean(x) - qt * sd(x) / sqrt(n))
})
UCL <- replicate(m, expr = {
  x <- rchisq(n, 2)
  return(mean(x) + qt * sd(x) / sqrt(n))
})
mean((LCL < 2) * (UCL > 2))
```

## Exercise 7.10

Simulation code and density histogram.

```{r, fig.align='center', fig.cap='Density histograms of the replicates in each case.'}
N <- 100
M <- 1000

gini.ratio <- function(x) {
  xi <- numeric(N)
  for (i in 1:N) {
    xi[i] <- (2 * i - N - 1) * x[i]
  }
  sum(xi) / (N * N * mean(x))
}

# standard lognormal
gini.log  <- replicate(M, expr = {
  x <- sort(rlnorm(N))
  gini.ratio(x)
})

# uniform distribution
gini.uni <-  replicate(M, expr = {
  x <- sort(runif(N))
  gini.ratio(x)
})

# Bernoulli(0.1)
gini.ber <-  replicate(M, expr = {
  x <- sort(rbernoulli(N, p = 0.1))
  gini.ratio(x)
})

# Mean, median
result = list(
  mean = c(
    lognormal = mean(gini.log),
    uniform = mean(gini.uni),
    bernoulli = mean(gini.ber)
  ),
  median = c(
    lognormal = median(gini.log),
    uniform = median(gini.uni),
    bernoulli = median(gini.ber)
  )
) %>% as.data.frame()

result

declies <- seq(0.1, 1, 0.1)
list(
  lognormal = quantile(gini.log, declies),
  uniform = quantile(gini.uni, declies),
  bernoulli = quantile(gini.ber, declies)
) %>% as.data.frame() %>% t()

par(mfrow=c(2,2))
hist(gini.log, probability = TRUE, main = "Histogram (using Log-normal)")
hist(gini.uni, probability = TRUE, main = "Histogram (using Uniform)")
hist(gini.ber, probability = TRUE, main = "Histogram (using Bernoulli)")
```

## Project 7.D

Comparison the kurtosis tests of multivariate normality with the energy test of multivariate normality $mvnorm.etest$ (energy)

```{r}
alpha <- .1
n <- 30
M <- 2500
energy_test <- kutosis_test <- numeric(M)
d <- 2

# calculate kurtosis statistic
kurtosis <- function(x) {
  b <- numeric(n)
  x_ <- matrix(colMeans(x), nrow =  d, ncol =  1)
  for (i in 1:n) {
    xi <- matrix(x[i, ], nrow = d, ncol = 1)
    b[i] <- t(xi - x_) %*% solve(var(x)) %*% (xi - x_)
  }
  sum(b ^ 2) / n
}

# estimate the kurtosis
for (i in 1:M) {
  x <- mlbench.twonorm(n = n, d = d)$x
  kutosis_test[i] <- as.integer(abs((kurtosis(x) - d * (d + 2)) / sqrt(8 * d * (d + 2) / n)) >= qnorm(1 - alpha / 2))
  energy_test[i] <- as.integer(mvnorm.etest(x, R = 200)$p.value <= alpha)
}
cbind(kurtosis = mean(kutosis_test), energy = mean(energy_test))
```